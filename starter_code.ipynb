{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1： 模型理论与应用\n",
    "在第一部分主要以证明和推导为主。以下几个问题都是比较经典的问题，会对模型的深入理解会有很大的帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 逻辑回归相关 (30分)\n",
    "假设我们有训练数据$D=\\{(\\mathbf{x}_1,y_1),...,(\\mathbf{x}_n,y_n)\\}$, 其中$(\\mathbf{x}_i,y_i)$为每一个样本，而且$\\mathbf{x}_i$是样本的特征并且$\\mathbf{x}_i\\in \\mathcal{R}^D$, $y_i$代表样本数据的标签（label）, 取值为$0$或者$1$. 在逻辑回归中，模型的参数为$(\\mathbf{w},b)$。对于向量，我们一般用粗体来表达。请回答以下问题。最好用Markdown自带的Latex来编写。（如果实在不行，可以手写然后拍照完放入word或者转成PDF，作为独立的文件来提交）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 在逻辑回归模型下，请写出目标函数（objective function）, 也就是我们需要\"最小化\"的目标（也称之为损失函数或者loss function)，不需要考虑正则 （3分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(\\mathbf{w},b)= -\\sum_{i=0}^n y_i\\log{p(y_i=1|x_i,w,b)}+(1-y_i)\\log{1-p(y_i=1|x_i,w,b)}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 求出$L(\\mathbf{w},b)$的梯度（或者计算导数），需要必要的中间过程。（5分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L(\\mathbf{w},b)}{\\partial \\mathbf{w}}=\\sum_{i=0}^n [\\sigma(w^Tx_i+b)-y_i]x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L(\\mathbf{w},b)}{\\partial b}=\\sum_{i=0}^n [\\sigma(w^Tx_i+b)-y_i]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 请写出基于梯度下降法（batch）的对于$\\mathbf{w}$和$b$的更新 （5分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{t+1}=w^t - \\eta\\sum_{i=0}^m\\frac{\\partial L(w,b)}{\\partial w} $\n",
    "\n",
    "$b^{t+1}=w^t - \\eta\\sum_{i=0}^m\\frac{\\partial L(w,b)}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 假设在(a)的基础上加了一个L2正则项，请写出基于梯度下降法（batch）的对于$\\mathbf{w}$和$b$的更新 （5分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{t+1}=w^t - \\eta\\sum_{i=0}^m\\frac{\\partial L(w,b)}{\\partial w} + w_t $\n",
    "\n",
    "$b^{t+1}=w^t - \\eta\\sum_{i=0}^m\\frac{\\partial L(w,b)}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们来证明逻辑回归函数是凸函数。假设一个函数是凸函数，我们则可以得出局部最优解即为全局最优解，所以假设我们通过随机梯度下降法等手段找到最优解\n",
    "时我们就可以确认这个解就是全局最优解。证明凸函数的方法有很多种，在这里我们介绍一种方法，就是基于二次求导大于等于0。比如给定一个函数$f(x)=x^2-3x+3$，做两次\n",
    "求导之后即可以得出$f''(x)=2 > 0$，所以这个函数就是凸函数。类似的，这种理论也应用于多元变量中的函数上。在多元函数上，只要证明二阶导数是posititive semidefinite即可以。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) 在(b)的基础上接着对$\\mathbf{w}$求导（等于二阶导数，二阶导数的维度为$D\\times D$），这个二阶导数也称之为Hessian Matrix(https://en.wikipedia.org/wiki/Hessian_matrix) 对于矩阵、向量的求导请参考：https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf （8分）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial^2 \\mathcal{L}}{\\partial^2 \\mathbf{w}}=$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) 请说明在(e)的得出来的Hessian Matrix是Positive Definite. 提示：为了证明一个$D\\times D$的矩阵$H$为Positive Semidefinite，需要证明对于任意一个非零向量$v\\in \\mathcal{R}^D$, 需要得出$v^{T}Hv >=0$ （4分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请推导或者说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2： 情感分析项目 (70分)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目的目标是基于用户提供的评论，通过算法自动去判断其评论是正面的还是负面的情感。比如给定一个用户的评论：\n",
    "- 评论1： “我特别喜欢这个电器，我已经用了3个月，一点问题都没有！”\n",
    "- 评论2： “我从这家淘宝店卖的东西不到一周就开始坏掉了，强烈建议不要买，真实浪费钱”\n",
    "\n",
    "对于这两个评论，第一个明显是正面的，第二个是负面的。 我们希望搭建一个AI算法能够自动帮我们识别出评论是正面还是负面。\n",
    "\n",
    "情感分析的应用场景非常丰富，也是NLP技术在不同场景中落地的典范。比如对于一个证券领域，作为股民，其实比较关注舆论的变化，这个时候如果能有一个AI算法自动给网络上的舆论做正负面判断，然后把所有相关的结论再整合，这样我们可以根据这些大众的舆论，辅助做买卖的决策。 另外，在电商领域评论无处不在，而且评论已经成为影响用户购买决策的非常重要的因素，所以如果AI系统能够自动分析其情感，则后续可以做很多有意思的应用。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分析是文本处理领域经典的问题。整个系统一般会包括几个模块：\n",
    "- 数据的抓取： 通过爬虫的技术去网络抓取相关文本数据\n",
    "- 数据的清洗/预处理：在本文中一般需要去掉无用的信息，比如各种标签（HTML标签），标点符号，停用词等等\n",
    "- 把文本信息转换成向量： 这也成为特征工程，文本本身是不能作为模型的输入，只有数字（比如向量）才能成为模型的输入。所以进入模型之前，任何的信号都需要转换成模型可识别的数字信号（数字，向量，矩阵，张量...)\n",
    "- 选择合适的模型以及合适的评估方法。 对于情感分析来说，这是二分类问题（或者三分类：正面，负面，中性），所以需要采用分类算法比如逻辑回归，朴素贝叶斯，神经网络，SVM等等。另外，我们需要选择合适的评估方法，比如对于一个应用，我们是关注准确率呢，还是关注召回率呢？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本次项目中，我们已经给定了训练数据和测试数据，它们分别是 train.positive.txt, train.negative.txt， test_combined.txt. 请注意训练数据和测试数据的格式不一样，详情请见文件内容。 整个项目你需要完成以下步骤：\n",
    "\n",
    "数据的读取以及清洗： 从给定的.txt中读取内容，并做一些数据清洗，这里需要做几个工作： （1） 文本的读取，需要把字符串内容读进来。 （2）去掉无用的字符比如标点符号，多余的空格，换行符等 （3） 分词\n",
    "把文本转换成TF-IDF向量： 这部分直接可以利用sklearn提供的TfidfVectorizer类来做。\n",
    "利用逻辑回归模型来做分类，并通过交叉验证选择最合适的超参数\n",
    "利用支持向量机做分类，并通过交叉验证选择神经网络的合适的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Reading: 文本读取 （5分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file():\n",
    "    \"\"\"\n",
    "    读取训练数据和测试数据，并对它们做一些预处理\n",
    "    \"\"\"    \n",
    "    train_pos_file = \"data/train.positive.txt\"\n",
    "    train_neg_file = \"data/train.negative.txt\"\n",
    "    test_comb_file = \"data/test.combined.txt\"\n",
    "    \n",
    "    # TODO: 读取文件部分，把具体的内容写入到变量里面\n",
    "    train_comments = []\n",
    "    train_labels = []\n",
    "    test_comments = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for line in open(train_pos_file):\n",
    "        if len(line.rstrip())!=0 and (\"<review\" not in line and  \"</review>\" not in line):\n",
    "            train_comments.append(line.rstrip())\n",
    "            train_labels.append(1)\n",
    "            \n",
    "    for line in open(train_neg_file):\n",
    "        if len(line.rstrip())!=0 and (\"<review\" not in line and  \"</review>\" not in line):\n",
    "            train_comments.append(line.rstrip())\n",
    "            train_labels.append(0)\n",
    "            \n",
    "    for line in open(test_comb_file):\n",
    "        if len(line.rstrip())!=0 and (\"<review\" not in line and  \"</review>\" not in line):\n",
    "            test_comments.append(line.rstrip())\n",
    "        if \"<review\" in line:\n",
    "            linestrip = line.rstrip()\n",
    "            test_labels.append(int(linestrip[-3]))\n",
    "            \n",
    "    return train_comments,train_labels,test_comments,test_labels\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorary Analysis: 做一些简单的可视化分析 （10分） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12557 2500\n",
      "12557 2500\n"
     ]
    }
   ],
   "source": [
    "# 训练数据和测试数据大小\n",
    "train_comments, train_labels, test_comments, test_labels = process_file()\n",
    "print (len(train_comments), len(test_comments))\n",
    "print (len(train_labels), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/lx/h1cvbsnx1cq2_bnv5d7cltsh0000gn/T/jieba.cache\n",
      "Loading model cost 0.600 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# TODO: 对于训练数据中的正负样本，分别画出一个histogram， histogram的x抽是每一个样本中字符串的长度，y轴是拥有这个长度的样本的百分比。\n",
    "#       并说出样本长度是否对情感有相关性 (需要先用到结巴分词)\n",
    "#       参考：https://en.wikipedia.org/wiki/Histogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "\n",
    "pos_comment_size = []\n",
    "neg_comment_size = []\n",
    "\n",
    "for train_comment, train_label in zip(train_comments, train_labels):\n",
    "    seg_list = list(jieba.cut(train_comment.rstrip(), cut_all=False))\n",
    "    if train_label == 1:\n",
    "        pos_comment_size.append(len(seg_list))\n",
    "    if train_label == 0:\n",
    "        neg_comment_size.append(len(seg_list))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.68371261e-01, 1.82050626e-01, 6.73740731e-02, 3.29838916e-02,\n",
       "        1.72590130e-02, 1.11224751e-02, 5.62515981e-03, 3.32395807e-03,\n",
       "        2.04551266e-03, 1.53413449e-03, 1.66197903e-03, 7.67067246e-04,\n",
       "        5.11378164e-04, 6.39222705e-04, 5.11378164e-04, 3.83533623e-04,\n",
       "        5.11378164e-04, 1.53413449e-03, 1.15060087e-03, 2.55689082e-04,\n",
       "        2.55689082e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.27844541e-04]),\n",
       " array([  1.        ,  21.03333333,  41.06666667,  61.1       ,\n",
       "         81.13333333, 101.16666667, 121.2       , 141.23333333,\n",
       "        161.26666667, 181.3       , 201.33333333, 221.36666667,\n",
       "        241.4       , 261.43333333, 281.46666667, 301.5       ,\n",
       "        321.53333333, 341.56666667, 361.6       , 381.63333333,\n",
       "        401.66666667, 421.7       , 441.73333333, 461.76666667,\n",
       "        481.8       , 501.83333333, 521.86666667, 541.9       ,\n",
       "        561.93333333, 581.96666667, 602.        ]),\n",
       " <a list of 30 Patch objects>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pos_comment_size,bins=30,weights=[1./len(pos_comment_size)]*len(pos_comment_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.16473073e-01, 2.77296727e-01, 1.28827878e-01, 6.75818374e-02,\n",
       "        3.35797254e-02, 2.82998944e-02, 1.54171067e-02, 1.03484688e-02,\n",
       "        5.06863780e-03, 3.80147835e-03, 2.53431890e-03, 2.53431890e-03,\n",
       "        1.90073918e-03, 8.44772967e-04, 1.47835269e-03, 2.11193242e-04,\n",
       "        6.33579725e-04, 6.33579725e-04, 2.11193242e-04, 4.22386484e-04,\n",
       "        2.11193242e-04, 2.11193242e-04, 2.11193242e-04, 0.00000000e+00,\n",
       "        8.44772967e-04, 2.11193242e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.11193242e-04]),\n",
       " array([  1.,  14.,  27.,  40.,  53.,  66.,  79.,  92., 105., 118., 131.,\n",
       "        144., 157., 170., 183., 196., 209., 222., 235., 248., 261., 274.,\n",
       "        287., 300., 313., 326., 339., 352., 365., 378., 391.]),\n",
       " <a list of 30 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(neg_comment_size,bins=30,weights=[1./len(neg_comment_size)]*len(neg_comment_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('不错', 1076), ('买', 950), ('喜欢', 915), ('没有', 579), ('非常', 556), ('一个', 505), ('卓越', 504), ('本书', 475), ('书', 443), ('会', 415), ('说', 407), ('感觉', 387), ('中', 357), ('the', 339), ('…', 322), ('很多', 309), ('听', 309), ('质量', 306), ('内容', 287), ('现在', 280)]\n",
      "[('买', 870), ('没有', 692), ('卓越', 535), ('说', 487), ('没', 275), ('质量', 270), ('一个', 258), ('太', 256), ('喜欢', 244), ('东西', 243), ('问题', 241), ('现在', 236), ('感觉', 230), ('内容', 218), ('知道', 217), ('书', 214), ('听', 211), ('想', 211), ('不能', 198), ('手机', 196)]\n"
     ]
    }
   ],
   "source": [
    "# TODO： 分别列出训练数据中正负样本里的top 20单词（可以做适当的stop words removal）。 \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop_words = \"data/cn_stopwords.txt\"\n",
    "stop_words_list = []\n",
    "for line in open(stop_words):\n",
    "    stop_word = line.strip()\n",
    "    stop_words_list.append(stop_word)\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "pos_word_count=[]\n",
    "neg_word_count=[]\n",
    "\n",
    "\n",
    "\n",
    "for train_comment, train_label in zip(train_comments, train_labels):\n",
    "    l=str.maketrans('','',string.punctuation)\n",
    "    train_comment = str(train_comment).translate(l)\n",
    "    \n",
    "    seg_list = list(jieba.cut(train_comment.rstrip(), cut_all=False))\n",
    "    \n",
    "    for word in seg_list:\n",
    "        word = word.rstrip()\n",
    "        if word not in stop_words_list and len(word)>0:\n",
    "            if train_label==1:\n",
    "                pos_word_count.append(word)\n",
    "            else:\n",
    "                neg_word_count.append(word)\n",
    "                \n",
    "counter1 = Counter(pos_word_count)\n",
    "word_freq1 = counter1.most_common(20)\n",
    "counter2 = Counter(neg_word_count)\n",
    "word_freq2 = counter2.most_common(20)\n",
    "print(word_freq1)\n",
    "print(word_freq2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Cleaning: 文本处理部分 （10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12557\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "# TODO：对于train_comments, test_comments进行字符串的处理，几个考虑的点：\n",
    "#   1. 停用词过滤\n",
    "#   2. 去掉特殊符号\n",
    "#   3. 去掉数字（比如价格..)\n",
    "#   4. ...\n",
    "#   需要注意的点是，由于评论数据本身很短，如果去掉的太多，很可能字符串长度变成0\n",
    "#   预处理部分，可以自行选择合适的方.\n",
    "train_comments_new = [] \n",
    "test_comments_new = []\n",
    "\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for train_comment in train_comments:\n",
    "    l=str.maketrans('','',string.punctuation)\n",
    "    train_comment = str(train_comment).translate(l)\n",
    "    \n",
    "    train_seg_list = list(jieba.cut(train_comment.rstrip(), cut_all=False))\n",
    "    \n",
    "    \n",
    "    train_words_list = []\n",
    "\n",
    "    \n",
    "    for word in train_seg_list:\n",
    "        word = word.rstrip()\n",
    "        if word not in stop_words_list and not word.isdigit():\n",
    "            train_words_list.append(word)\n",
    "\n",
    "    new_train_comment = ' '.join(train_words_list)\n",
    "    train_comments_new.append(new_train_comment)\n",
    "    \n",
    "for test_comment in test_comments:\n",
    "    l=str.maketrans('','',string.punctuation)\n",
    "    train_comment = str(train_comment).translate(l)\n",
    "    \n",
    "    test_seg_list = list(jieba.cut(test_comment.rstrip(), cut_all=False))\n",
    "    \n",
    "\n",
    "    test_words_list = []\n",
    "    \n",
    "    for word in test_seg_list:\n",
    "        word = word.rstrip()\n",
    "        if word not in stop_words_list and not word.isdigit():\n",
    "            test_words_list.append(word)\n",
    "    new_test_comment = ' '.join(test_words_list)\n",
    "    test_comments_new.append(new_test_comment)\n",
    "\n",
    "print(len(train_comments_new))\n",
    "print(len(test_comments_new))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction : 从文本中提取特征 （10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12557, 26627) (2500, 26627) (12557,) (2500,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: 利用tf-idf从文本中提取特征,写到数组里面. \n",
    "#       参考：https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_comments_new) # 训练数据的特征\n",
    "y_train = train_labels # 训练数据的label\n",
    "X_test =  vectorizer.transform(test_comments_new) # 测试数据的特征\n",
    "y_test =  test_labels # 测试数据的label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling: 训练模型以及选择合适的超参数 （25分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# TODO： 利用逻辑回归来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       逻辑回归的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#       对于逻辑回归，经常调整的超参数为： C\n",
    "parameters = {'C':[0.00001, 0.0001, 0.001, 0.005,0.01,0.05, 0.1, 0.5,1,2,5,10]}\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(lr, parameters, cv=10,n_jobs=10)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "print (clf.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 684  566]\n",
      " [ 130 1120]]\n",
      "precision:  0.66429418742586\n",
      "recall:  0.896\n",
      "accuracy:  0.7216\n",
      "f1 score:  0.7629427792915531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score,f1_score\n",
    "predictions = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "print(\"precision: \", precision_score(y_test,predictions))\n",
    "print(\"recall: \", recall_score(y_test,predictions))\n",
    "print(\"accuracy: \", accuracy_score(y_test,predictions))\n",
    "print(\"f1 score: \", f1_score(y_test,predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray()\n",
    "X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# TODO： 利用SVM来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       SVM的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#       对于SVM模型，经常调整的超参数为：C, gamma, kernel\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C':[1,2,4], 'gamma':['scale','auto'], 'kernel':('linear', 'poly', 'rbf', 'sigmoid')\n",
    "        }\n",
    "svc = svm.SVC()\n",
    "\n",
    "svc.fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "clf = GridSearchCV(svc, parameters, cv=10,n_jobs=10)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "print (clf.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 695  555]\n",
      " [ 141 1109]]\n",
      "precision:  0.6664663461538461\n",
      "recall:  0.8872\n",
      "accuracy:  0.7216\n",
      "f1 score:  0.7611530542210019\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score,f1_score\n",
    "predictions = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "print(\"precision: \", precision_score(y_test,predictions))\n",
    "print(\"recall: \", recall_score(y_test,predictions))\n",
    "print(\"accuracy: \", accuracy_score(y_test,predictions))\n",
    "print(\"f1 score: \", f1_score(y_test,predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于超参数的调整，我们经常使用gridsearch，这也是工业界最常用的方法，但它的缺点是需要大量的计算，所以近年来这方面的研究也成为了重点。 其中一个比较经典的成果为Bayesian Optimization（利用贝叶斯的思路去寻找最好的超参数）。Ryan P. Adams主导的Bayesian Optimization利用高斯过程作为后验概率（posteior distribution）来寻找最优解。 https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf 在下面的练习中，我们尝试使用Bayesian Optimization工具来去寻找最优的超参数。参考工具：https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   expC    | expGamma  |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.763   \u001b[0m | \u001b[0m-2.042   \u001b[0m | \u001b[0m 0.1105  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6953  \u001b[0m | \u001b[0m-0.8114  \u001b[0m | \u001b[0m 0.9268  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.7738  \u001b[0m | \u001b[95m 0.8999  \u001b[0m | \u001b[95m-1.637   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.6786  \u001b[0m | \u001b[0m-1.618   \u001b[0m | \u001b[0m 1.009   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6337  \u001b[0m | \u001b[0m 1.791   \u001b[0m | \u001b[0m 1.38    \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7729  \u001b[0m | \u001b[0m 0.9138  \u001b[0m | \u001b[0m-1.392   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7627  \u001b[0m | \u001b[0m-1.337   \u001b[0m | \u001b[0m-1.204   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7636  \u001b[0m | \u001b[0m-3.0     \u001b[0m | \u001b[0m-0.9785  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7634  \u001b[0m | \u001b[0m-0.5309  \u001b[0m | \u001b[0m-3.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7701  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m-3.0     \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7415  \u001b[0m | \u001b[0m-2.814   \u001b[0m | \u001b[0m-3.0     \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7419  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m-1.603   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.764   \u001b[0m | \u001b[0m 0.8238  \u001b[0m | \u001b[0m-2.999   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7634  \u001b[0m | \u001b[0m-0.1242  \u001b[0m | \u001b[0m-1.464   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.759   \u001b[0m | \u001b[0m-3.0     \u001b[0m | \u001b[0m 0.1492  \u001b[0m |\n",
      "=================================================\n",
      "Final result: {'target': 0.7737690110172353, 'params': {'expC': 0.8998790405940174, 'expGamma': -1.6370369735867918}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target': 0.7737690110172353,\n",
       " 'params': {'expC': 0.8998790405940174, 'expGamma': -1.6370369735867918}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: 仍然使用SVM模型，但在这里使用Bayesian Optimization来寻找最好的超参数。 \n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用Bayesian Optimization https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       SVM的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#       对于SVM模型，经常调整的超参数为：C, gamma, kernel\n",
    "#       参考Bayesian Optimization开源工具： https://github.com/fmfn/BayesianOptimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def svc_cv(C, gamma, data, targets):\n",
    "    \"\"\"SVC cross validation.\n",
    "    This function will instantiate a SVC classifier with parameters C and\n",
    "    gamma. Combined with data and targets this will in turn be used to perform\n",
    "    cross validation. The result of cross validation is returned.\n",
    "    Our goal is to find combinations of C and gamma that maximizes the roc_auc\n",
    "    metric.\n",
    "    \"\"\"\n",
    "    estimator = svm.SVC(C=C, gamma=gamma, random_state=2)\n",
    "    cval = cross_val_score(estimator, data, targets, scoring='roc_auc', cv=10)\n",
    "    return cval.mean()\n",
    "\n",
    "\n",
    "def optimize_svc(data, targets):\n",
    "    \"\"\"Apply Bayesian Optimization to SVC parameters.\"\"\"\n",
    "    def svc_crossval(expC, expGamma):\n",
    "        \"\"\"Wrapper of SVC cross validation.\n",
    "        Notice how we transform between regular and log scale. While this\n",
    "        is not technically necessary, it greatly improves the performance\n",
    "        of the optimizer.\n",
    "        \"\"\"\n",
    "        C = 10 ** expC\n",
    "        gamma = 10 ** expGamma\n",
    "        return svc_cv(C=C, gamma=gamma, data=data, targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=svc_crossval,\n",
    "        pbounds={\"expC\": (-3, 2), \"expGamma\": (-3, 2) },\n",
    "        random_state=1234,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(n_iter=10)\n",
    "\n",
    "    print(\"Final result:\", optimizer.max)\n",
    "    return optimizer.max\n",
    "\n",
    "optimize_svc(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6892"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C=10** 0.8998790405940174\n",
    "gamma=10** (-1.6370369735867918)\n",
    "svc = svm.SVC(C=C, gamma=gamma)\n",
    "svc.fit(X_train,y_train).score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 538  712]\n",
      " [  65 1185]]\n",
      "precision:  0.6246705324196099\n",
      "recall:  0.948\n",
      "accuracy:  0.6892\n",
      "f1 score:  0.7530981887511916\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"precision: \", precision_score(y_test,predictions))\n",
    "print(\"recall: \", recall_score(y_test,predictions))\n",
    "print(\"accuracy: \", accuracy_score(y_test,predictions))\n",
    "print(\"f1 score: \", f1_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 特征: 添加n-gram特征 (10分)\n",
    "在原有tf-idf特征的基础上，添加n-gram特征（在这里我们使用bi-gram特征）。添加完之后效果是否有提升？ 为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12557, 128567) (2500, 128567) (12557,) (2500,)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_train = vectorizer.fit_transform(train_comments_new) # 训练数据的特征\n",
    "y_train = train_labels # 训练数据的label\n",
    "X_test =  vectorizer.transform(test_comments_new) # 测试数据的特征\n",
    "y_test =  test_labels # 测试数据的label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# TODO： 利用逻辑回归来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       逻辑回归的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#       对于逻辑回归，经常调整的超参数为： C\n",
    "parameters = {'C':[0.00001, 0.0001, 0.001, 0.005,0.01,0.05, 0.1, 0.5,1,2,5,10]}\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(lr, parameters, cv=10)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "print (clf.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 734  516]\n",
      " [ 140 1110]]\n",
      "precision:  0.6826568265682657\n",
      "recall:  0.888\n",
      "accuracy:  0.7376\n",
      "f1 score:  0.7719054242002781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score,f1_score\n",
    "predictions = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "print(\"precision: \", precision_score(y_test,predictions))\n",
    "print(\"recall: \", recall_score(y_test,predictions))\n",
    "print(\"accuracy: \", accuracy_score(y_test,predictions))\n",
    "print(\"f1 score: \", f1_score(y_test,predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   expC    | expGamma  |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7624  \u001b[0m | \u001b[0m-2.042   \u001b[0m | \u001b[0m 0.1105  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7164  \u001b[0m | \u001b[0m-0.8114  \u001b[0m | \u001b[0m 0.9268  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.7716  \u001b[0m | \u001b[95m 0.8999  \u001b[0m | \u001b[95m-1.637   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7029  \u001b[0m | \u001b[0m-1.618   \u001b[0m | \u001b[0m 1.009   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6328  \u001b[0m | \u001b[0m 1.791   \u001b[0m | \u001b[0m 1.38    \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7653  \u001b[0m | \u001b[0m-0.06297 \u001b[0m | \u001b[0m-1.32    \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.765   \u001b[0m | \u001b[0m-2.208   \u001b[0m | \u001b[0m-1.465   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7676  \u001b[0m | \u001b[0m 0.04026 \u001b[0m | \u001b[0m-3.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7684  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m-3.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7463  \u001b[0m | \u001b[0m-3.0     \u001b[0m | \u001b[0m-3.0     \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7619  \u001b[0m | \u001b[0m-3.0     \u001b[0m | \u001b[0m-0.4843  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7606  \u001b[0m | \u001b[0m-1.38    \u001b[0m | \u001b[0m-2.99    \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7588  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m-1.679   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7644  \u001b[0m | \u001b[0m-1.302   \u001b[0m | \u001b[0m-0.7478  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7677  \u001b[0m | \u001b[0m 1.062   \u001b[0m | \u001b[0m-2.63    \u001b[0m |\n",
      "=================================================\n",
      "Final result: {'target': 0.7715758441281476, 'params': {'expC': 0.8998790405940174, 'expGamma': -1.6370369735867918}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target': 0.7715758441281476,\n",
       " 'params': {'expC': 0.8998790405940174, 'expGamma': -1.6370369735867918}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: 仍然使用SVM模型，但在这里使用Bayesian Optimization来寻找最好的超参数。 \n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用Bayesian Optimization https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       SVM的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#       对于SVM模型，经常调整的超参数为：C, gamma, kernel\n",
    "#       参考Bayesian Optimization开源工具： https://github.com/fmfn/BayesianOptimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def svc_cv(C, gamma, data, targets):\n",
    "    \"\"\"SVC cross validation.\n",
    "    This function will instantiate a SVC classifier with parameters C and\n",
    "    gamma. Combined with data and targets this will in turn be used to perform\n",
    "    cross validation. The result of cross validation is returned.\n",
    "    Our goal is to find combinations of C and gamma that maximizes the roc_auc\n",
    "    metric.\n",
    "    \"\"\"\n",
    "    estimator = svm.SVC(C=C, gamma=gamma, random_state=2)\n",
    "    cval = cross_val_score(estimator, data, targets, scoring='roc_auc', cv=10)\n",
    "    return cval.mean()\n",
    "\n",
    "\n",
    "def optimize_svc(data, targets):\n",
    "    \"\"\"Apply Bayesian Optimization to SVC parameters.\"\"\"\n",
    "    def svc_crossval(expC, expGamma):\n",
    "        \"\"\"Wrapper of SVC cross validation.\n",
    "        Notice how we transform between regular and log scale. While this\n",
    "        is not technically necessary, it greatly improves the performance\n",
    "        of the optimizer.\n",
    "        \"\"\"\n",
    "        C = 10 ** expC\n",
    "        gamma = 10 ** expGamma\n",
    "        return svc_cv(C=C, gamma=gamma, data=data, targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=svc_crossval,\n",
    "        pbounds={\"expC\": (-3, 2), \"expGamma\": (-3, 2) },\n",
    "        random_state=1234,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(n_iter=10)\n",
    "\n",
    "    print(\"Final result:\", optimizer.max)\n",
    "    return optimizer.max\n",
    "\n",
    "optimize_svc(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.686"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C=10** 0.8998790405940174\n",
    "gamma=10** (-1.6370369735867918)\n",
    "svc = svm.SVC(C=C, gamma=gamma)\n",
    "svc.fit(X_train,y_train).score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 734  516]\n",
      " [ 140 1110]]\n",
      "precision:  0.6826568265682657\n",
      "recall:  0.888\n",
      "accuracy:  0.7376\n",
      "f1 score:  0.7719054242002781\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"precision: \", precision_score(y_test,predictions))\n",
    "print(\"recall: \", recall_score(y_test,predictions))\n",
    "print(\"accuracy: \", accuracy_score(y_test,predictions))\n",
    "print(\"f1 score: \", f1_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000ma',\n",
       " '1000ma 待机',\n",
       " '1000rpm',\n",
       " '100g',\n",
       " '100g 音乐',\n",
       " '100ml',\n",
       " '100ml 150ml',\n",
       " '100rmb',\n",
       " '100rmb 这才',\n",
       " '1024x576',\n",
       " '1024x576 一定',\n",
       " '1024x576 支持',\n",
       " '1024x576 高清',\n",
       " '1050mah',\n",
       " '1050mah 可能',\n",
       " '1080p',\n",
       " '1080p 感觉',\n",
       " '10cd',\n",
       " '10cd 精装',\n",
       " '10dv',\n",
       " '10dv d版',\n",
       " '10dvd',\n",
       " '10dvd 画面质量',\n",
       " '11l',\n",
       " '120fps',\n",
       " '120fps 录像',\n",
       " '128kbps',\n",
       " '128kbps 很多歌',\n",
       " '128m',\n",
       " '128m 手机',\n",
       " '128mb',\n",
       " '128mb 一个',\n",
       " '12qt',\n",
       " '12vcdmp3',\n",
       " '12vcdmp3 节目',\n",
       " '1388rmb',\n",
       " '14dvd',\n",
       " '14dvd 另款',\n",
       " '14dvd 碟片',\n",
       " '14g',\n",
       " '14g 画质',\n",
       " '14minute',\n",
       " '14minute vehicle',\n",
       " '150ml',\n",
       " '150ml 选择',\n",
       " '15cd',\n",
       " '15cd 张学友',\n",
       " '15m',\n",
       " '15m 大层',\n",
       " '15ml',\n",
       " '15xx',\n",
       " '15xx 23xx',\n",
       " '163com',\n",
       " '163com 唐朝',\n",
       " '16k',\n",
       " '16k 厚实',\n",
       " '175rmb',\n",
       " '175rmb 去掉',\n",
       " '1800ma',\n",
       " '1800ma 电池',\n",
       " '183club',\n",
       " '183club 支持',\n",
       " '186g',\n",
       " '186g 文件',\n",
       " '186g 这是',\n",
       " '1999s',\n",
       " '1999s bailamos',\n",
       " '1a',\n",
       " '1a 市售',\n",
       " '1cm',\n",
       " '1cm 裂纹',\n",
       " '1db',\n",
       " '1dvd',\n",
       " '1g',\n",
       " '1g 一倍',\n",
       " '1g 主板',\n",
       " '1g 内容',\n",
       " '1g 最大',\n",
       " '1g 现在',\n",
       " '1g 起来',\n",
       " '1gt',\n",
       " '1gt 里面',\n",
       " '1k',\n",
       " '1k 文件',\n",
       " '1mp32003',\n",
       " '1mp32003 节目',\n",
       " '1st',\n",
       " '1vs1',\n",
       " '1vs1 1vs2',\n",
       " '1vs2',\n",
       " '1vs2 3vs5',\n",
       " '2000s',\n",
       " '2000s be',\n",
       " '2000s sad',\n",
       " '2000w',\n",
       " '2000w 电暖气',\n",
       " '2001s',\n",
       " '2001s hero',\n",
       " '2003tvb',\n",
       " '2003tvb 台庆剧',\n",
       " '2004s',\n",
       " '2004s not',\n",
       " '2007s',\n",
       " '2007s do',\n",
       " '2007word',\n",
       " '2007word 方便',\n",
       " '200g',\n",
       " '200g 桔红色',\n",
       " '200g 移动硬盘',\n",
       " '200g 管装',\n",
       " '212g',\n",
       " '212g 罐装',\n",
       " '21hifi',\n",
       " '21hifi 试听',\n",
       " '21st',\n",
       " '21st century',\n",
       " '22million',\n",
       " '22million records',\n",
       " '23xx',\n",
       " '24bit',\n",
       " '24bit 96khz',\n",
       " '24cm',\n",
       " '24cm 26cm',\n",
       " '24g',\n",
       " '24g 盒装',\n",
       " '24m',\n",
       " '24m 附加',\n",
       " '256m',\n",
       " '256m 过来',\n",
       " '26cm',\n",
       " '2c',\n",
       " '2c d版',\n",
       " '2d',\n",
       " '2d 插件',\n",
       " '2d 画面',\n",
       " '2dvd',\n",
       " '2dvd 加送',\n",
       " '2dvd 圣经',\n",
       " '2dvd9',\n",
       " '2dvd9 14g',\n",
       " '2e',\n",
       " '2e 处理器',\n",
       " '2g',\n",
       " '2g sandisk',\n",
       " '2g 卡塞得',\n",
       " '2g 很多歌',\n",
       " '2g 摆明',\n",
       " '2g 玫瑰',\n",
       " '2g 看起来',\n",
       " '2g 纯属',\n",
       " '2g 记忆棒',\n",
       " '2gddr400',\n",
       " '2gddr400 内存',\n",
       " '2k',\n",
       " '2k 就够',\n",
       " '2k 相当于',\n",
       " '2l',\n",
       " '2l 表格',\n",
       " '2vcd',\n",
       " '2xx',\n",
       " '2xx 价格',\n",
       " '30g',\n",
       " '30g 其实',\n",
       " '310m',\n",
       " '310m 左右',\n",
       " '320g',\n",
       " '320g 市场价',\n",
       " '320kbps',\n",
       " '320kbps 专集',\n",
       " '320kbps 接近',\n",
       " '3500c',\n",
       " '3500c 购买',\n",
       " '35mm',\n",
       " '35mm 连线',\n",
       " '373gb',\n",
       " '373gb 4gb',\n",
       " '37oz',\n",
       " '3cd',\n",
       " '3cd 好多',\n",
       " '3d',\n",
       " '3d 人设',\n",
       " '3d 效果',\n",
       " '3d 立体',\n",
       " '3d 网络',\n",
       " '3d 迷宫',\n",
       " '3d9',\n",
       " '3d9 d5',\n",
       " '3db',\n",
       " '3db 专业',\n",
       " '3dvd',\n",
       " '3dvd 包括',\n",
       " '3g',\n",
       " '3g tdscdma',\n",
       " '3g 功能',\n",
       " '3g 发展',\n",
       " '3g 效果',\n",
       " '3g 版本',\n",
       " '3h',\n",
       " '3h 單元',\n",
       " '3k',\n",
       " '3k 出手',\n",
       " '3s',\n",
       " '3s 战士',\n",
       " '3vs5',\n",
       " '3vs5 7vs100',\n",
       " '40g',\n",
       " '40mbs',\n",
       " '40mbs 连续',\n",
       " '465g',\n",
       " '465g 左右',\n",
       " '470ml',\n",
       " '470ml 韩国',\n",
       " '480mbs',\n",
       " '4dvd',\n",
       " '4dvd 非常',\n",
       " '4g',\n",
       " '4g 2g',\n",
       " '4g tf',\n",
       " '4g 内存卡',\n",
       " '4g 卓越',\n",
       " '4g 朗科',\n",
       " '4g 特价',\n",
       " '4g4g',\n",
       " '4g4g 来看',\n",
       " '4gb',\n",
       " '4k',\n",
       " '4k 左右',\n",
       " '4vcd',\n",
       " '4vcd 一套',\n",
       " '4vcd 非常',\n",
       " '500g',\n",
       " '500g 实际',\n",
       " '500g 容量',\n",
       " '500g 玻璃瓶',\n",
       " '500ml',\n",
       " '500ml 竟然',\n",
       " '50cpu',\n",
       " '50cpu 占用率',\n",
       " '50m',\n",
       " '50ml',\n",
       " '50ml 产品价格',\n",
       " '50ml 瓶盖',\n",
       " '50rmb',\n",
       " '50rmb 以下',\n",
       " '512m',\n",
       " '512m 今天',\n",
       " '512m 卓越',\n",
       " '512m 可能',\n",
       " '512m 略感',\n",
       " '5130xm',\n",
       " '51r6',\n",
       " '51r6 已经',\n",
       " '55cm',\n",
       " '55cm 卓越',\n",
       " '5k',\n",
       " '5k 左右',\n",
       " '5v',\n",
       " '5v 1a',\n",
       " '5v1a',\n",
       " '5v1a 无法',\n",
       " '600k',\n",
       " '600k and',\n",
       " '6010c',\n",
       " '6010c 这机',\n",
       " '6070ml',\n",
       " '6070ml 碧优',\n",
       " '60s',\n",
       " '60s 没有',\n",
       " '6120c',\n",
       " '6120c 512m',\n",
       " '625g',\n",
       " '625g 图像',\n",
       " '6dvd',\n",
       " '6dvd 10dvd',\n",
       " '6dvd dvd',\n",
       " '6l',\n",
       " '6l 尺寸',\n",
       " '70s',\n",
       " '70s and',\n",
       " '75ml',\n",
       " '75ml 优惠',\n",
       " '77x2',\n",
       " '77x2 经典',\n",
       " '78db',\n",
       " '78db 之多',\n",
       " '7vs100',\n",
       " '7vs100 最后',\n",
       " '7vs100000',\n",
       " '7vs100000 吹上',\n",
       " '8010c',\n",
       " '80s',\n",
       " '80s the',\n",
       " '860mah',\n",
       " '860mah 东莞',\n",
       " '8cd',\n",
       " '8cd 3cd',\n",
       " '8dvd',\n",
       " '8dvd hd',\n",
       " '8dvd 经济',\n",
       " '8g',\n",
       " '8g 卡用',\n",
       " '8g 外观',\n",
       " '90s',\n",
       " '90s albums',\n",
       " '90s first',\n",
       " '96khz',\n",
       " '96khz 发烧级',\n",
       " '98hkd',\n",
       " '98hkd 其实',\n",
       " '9910c',\n",
       " '9910c 卓越',\n",
       " '99read',\n",
       " '99read 一次',\n",
       " 'a10',\n",
       " 'a10 售价',\n",
       " 'a1200',\n",
       " 'a1200 区别',\n",
       " 'a1200 很多',\n",
       " 'a1200 遗憾',\n",
       " 'a1800',\n",
       " 'a1800 似乎',\n",
       " 'a3',\n",
       " 'a3 本碟',\n",
       " 'a3100',\n",
       " 'a3100 第二天',\n",
       " 'a615',\n",
       " 'a615 两个',\n",
       " 'a630',\n",
       " 'a630 手机',\n",
       " 'a7719',\n",
       " 'a7719 手机',\n",
       " 'a805',\n",
       " 'a805 相比',\n",
       " 'a905',\n",
       " 'a905 手机',\n",
       " 'a930',\n",
       " 'a930 货到',\n",
       " 'ab',\n",
       " 'ab 不好',\n",
       " 'ab 血型',\n",
       " 'abandoned',\n",
       " 'abandoned the',\n",
       " 'abbey',\n",
       " 'abbey road',\n",
       " 'abc',\n",
       " 'abc 网上',\n",
       " 'abcd',\n",
       " 'abcd 嘴里',\n",
       " 'abcd 特别',\n",
       " 'abcd 简单',\n",
       " 'ability',\n",
       " 'ability to',\n",
       " 'abolitionists',\n",
       " 'abolitionists attempting',\n",
       " 'about',\n",
       " 'about finding',\n",
       " 'about his',\n",
       " 'about how',\n",
       " 'about it',\n",
       " 'about its',\n",
       " 'about the',\n",
       " 'about you',\n",
       " 'absent',\n",
       " 'absent here',\n",
       " 'absolutely',\n",
       " 'absolutely atrocious',\n",
       " 'abyss',\n",
       " 'abyss in',\n",
       " 'ac3',\n",
       " 'ac3 d5',\n",
       " 'academy',\n",
       " 'academy award',\n",
       " 'academy awardwinner',\n",
       " 'accent',\n",
       " 'accent for',\n",
       " 'accent he',\n",
       " 'accept',\n",
       " 'accept his',\n",
       " 'accompanying',\n",
       " 'accompanying ego',\n",
       " 'ace',\n",
       " 'ace combat',\n",
       " 'acer4710g',\n",
       " 'acer4710g 接上',\n",
       " 'achieved',\n",
       " 'achieved his',\n",
       " 'achieves',\n",
       " 'achieves surprisingly',\n",
       " 'acm',\n",
       " 'acm 绝对',\n",
       " 'acoustic',\n",
       " 'acoustic effort',\n",
       " 'acoustic guitar',\n",
       " 'across',\n",
       " 'across america',\n",
       " 'across the',\n",
       " 'acs120c2r',\n",
       " 'acs120c2r 相关',\n",
       " 'act',\n",
       " 'act when',\n",
       " 'act with',\n",
       " 'acting',\n",
       " 'acting would',\n",
       " 'action',\n",
       " 'action scenes',\n",
       " 'action sequence',\n",
       " 'action sequences',\n",
       " 'action thriller',\n",
       " 'action to',\n",
       " 'action with',\n",
       " 'actor',\n",
       " 'actor adds',\n",
       " 'actor and',\n",
       " 'actor portrays',\n",
       " 'actor that',\n",
       " 'actress',\n",
       " 'actress whose',\n",
       " 'actresses',\n",
       " 'actresses have',\n",
       " 'actual',\n",
       " 'actual music',\n",
       " 'actually',\n",
       " 'actually cruise',\n",
       " 'actually they',\n",
       " 'actually 封面',\n",
       " 'actually 经典',\n",
       " 'ad',\n",
       " 'ad oconnors',\n",
       " 'ad seeking',\n",
       " 'adams',\n",
       " 'adams 入门',\n",
       " 'adaptation',\n",
       " 'adaptation of',\n",
       " 'add',\n",
       " 'add atmosphere',\n",
       " 'add to',\n",
       " 'adding',\n",
       " 'adding more',\n",
       " 'adding solo',\n",
       " 'addition',\n",
       " 'addition to',\n",
       " 'adds',\n",
       " 'adds more',\n",
       " 'adds very',\n",
       " 'adobe',\n",
       " 'adobe 原装',\n",
       " 'adroitly',\n",
       " 'adroitly strikes',\n",
       " 'adult',\n",
       " 'adult contemporary',\n",
       " 'adultapproved',\n",
       " 'adultapproved pop',\n",
       " 'advantage',\n",
       " 'advantage of',\n",
       " 'adventure',\n",
       " 'adventure in',\n",
       " 'ae',\n",
       " 'ae 音响',\n",
       " 'affecting',\n",
       " 'affecting level',\n",
       " 'affectionate',\n",
       " 'affectionate reworking',\n",
       " 'affinity',\n",
       " 'affinity for',\n",
       " 'after',\n",
       " 'after algeria',\n",
       " 'after one',\n",
       " 'after purchase',\n",
       " 'after scoring',\n",
       " 'after seeing',\n",
       " 'after shortlived',\n",
       " 'after studying',\n",
       " 'after that',\n",
       " 'after the',\n",
       " 'afterand',\n",
       " 'afterand trading',\n",
       " 'afterlife',\n",
       " 'afterlife story',\n",
       " 'again',\n",
       " 'again assays',\n",
       " 'again cr',\n",
       " 'again enya',\n",
       " 'again one',\n",
       " 'again 叛逆',\n",
       " 'again 觉得',\n",
       " 'against',\n",
       " 'against his',\n",
       " 'against the',\n",
       " 'age',\n",
       " 'age and',\n",
       " 'age fortunecookie',\n",
       " 'age of',\n",
       " 'age 风格',\n",
       " 'aging',\n",
       " 'aging gary',\n",
       " 'agps',\n",
       " 'agps 需要',\n",
       " 'agresti',\n",
       " 'agresti opts',\n",
       " 'ai',\n",
       " 'ai 综合',\n",
       " 'airplane',\n",
       " 'airplane and',\n",
       " 'airport',\n",
       " 'airport 一点',\n",
       " 'airport 专辑',\n",
       " 'airport 我会',\n",
       " 'aishou',\n",
       " 'aishou dvd',\n",
       " 'akin',\n",
       " 'akin to',\n",
       " 'akira',\n",
       " 'akira kurosawa',\n",
       " 'akon',\n",
       " 'akon 合唱',\n",
       " 'al',\n",
       " 'al of',\n",
       " 'al pacino',\n",
       " 'alagia',\n",
       " 'alagia who',\n",
       " 'alan',\n",
       " 'alan cumming',\n",
       " 'alan 阿兰',\n",
       " 'album',\n",
       " 'album compiled',\n",
       " 'album could',\n",
       " 'album eventually',\n",
       " 'album had',\n",
       " 'album has',\n",
       " 'album inside',\n",
       " 'album not',\n",
       " 'album of',\n",
       " 'album that',\n",
       " 'album the',\n",
       " 'album tracks',\n",
       " 'album wholly',\n",
       " 'album 出现',\n",
       " 'album 限定',\n",
       " 'albums',\n",
       " 'albums as',\n",
       " 'albums on',\n",
       " 'albums released',\n",
       " 'albums went',\n",
       " 'albuquerque',\n",
       " 'albuquerque 研究',\n",
       " 'alejandro',\n",
       " 'alejandro agresti',\n",
       " 'alexander',\n",
       " 'alexander tcherepnin',\n",
       " 'alexandre',\n",
       " 'alexandre arcady',\n",
       " 'alexis',\n",
       " 'alexis bleidel',\n",
       " 'alfonso',\n",
       " 'alfonso cuar',\n",
       " 'alfred',\n",
       " 'alfred hitchcock',\n",
       " 'algeria',\n",
       " 'algeria on',\n",
       " 'algeria won',\n",
       " 'alive',\n",
       " 'alive at',\n",
       " 'all',\n",
       " 'all at',\n",
       " 'all fragile',\n",
       " 'all good',\n",
       " 'all his',\n",
       " 'all involved',\n",
       " 'all legs',\n",
       " 'all movie',\n",
       " 'all music',\n",
       " 'all night',\n",
       " 'all objective',\n",
       " 'all rightthanks',\n",
       " 'all that',\n",
       " 'all the',\n",
       " 'all these',\n",
       " 'all things',\n",
       " 'allegorical',\n",
       " 'allegorical tale',\n",
       " 'allegory',\n",
       " 'allegory for',\n",
       " 'allowing',\n",
       " 'allowing the',\n",
       " 'allows',\n",
       " 'allows the',\n",
       " 'almost',\n",
       " 'almost four',\n",
       " 'almost never',\n",
       " 'almost pages',\n",
       " 'almost pathetic',\n",
       " 'along',\n",
       " 'along with',\n",
       " 'along 首歌',\n",
       " 'aloud',\n",
       " 'already',\n",
       " 'already born',\n",
       " 'already completed',\n",
       " 'also',\n",
       " 'also be',\n",
       " 'also blake',\n",
       " 'also called',\n",
       " 'also does',\n",
       " 'also exerted',\n",
       " 'also features',\n",
       " 'also founded',\n",
       " 'also included',\n",
       " 'also letdown',\n",
       " 'also on',\n",
       " 'also one',\n",
       " 'also owes',\n",
       " 'also starring',\n",
       " 'also tone',\n",
       " 'alternative',\n",
       " 'alternative to',\n",
       " 'although',\n",
       " 'although gibsons',\n",
       " 'although it',\n",
       " 'although this',\n",
       " 'altogether',\n",
       " 'altogether that',\n",
       " 'always',\n",
       " 'always be',\n",
       " 'always is',\n",
       " 'always there',\n",
       " 'am',\n",
       " 'am 魏佳庆',\n",
       " 'amadeus',\n",
       " 'amadeus 演奏',\n",
       " 'amai',\n",
       " 'amai wana',\n",
       " 'amateur',\n",
       " 'amateur this',\n",
       " 'amazing',\n",
       " 'amazing movie',\n",
       " 'amazon',\n",
       " 'amazon 一下',\n",
       " 'amazon 专享',\n",
       " 'amazon 买不到',\n",
       " 'amazon 买过',\n",
       " 'amazon 包装箱',\n",
       " 'amazon 可否',\n",
       " 'amazon 可能',\n",
       " 'amazon 曲目',\n",
       " 'amazon 管理水平',\n",
       " 'amazon 编辑',\n",
       " 'amazoncn',\n",
       " 'amazoncn 今后',\n",
       " 'amazoncom',\n",
       " 'amber',\n",
       " 'amber tamblyn',\n",
       " 'ambiguity',\n",
       " 'ambiguity atop',\n",
       " 'ambiguity not',\n",
       " 'america',\n",
       " 'america but',\n",
       " 'america cinema',\n",
       " 'america ferrera',\n",
       " 'america which',\n",
       " 'american',\n",
       " 'american audiences',\n",
       " 'american films',\n",
       " 'american tenor',\n",
       " 'among',\n",
       " 'among the',\n",
       " 'among them',\n",
       " 'amour',\n",
       " 'amour 半弯',\n",
       " 'amour 淡雅',\n",
       " 'amply',\n",
       " 'amply proportioned',\n",
       " 'amuro',\n",
       " 'amuro namie',\n",
       " 'amy',\n",
       " 'amy back',\n",
       " 'amy kelly',\n",
       " 'an',\n",
       " 'an absolutely',\n",
       " 'an academy',\n",
       " 'an action',\n",
       " 'an actress',\n",
       " 'an ad',\n",
       " 'an affectionate',\n",
       " 'an afterlife',\n",
       " 'an aging',\n",
       " 'an album',\n",
       " 'an allegorical',\n",
       " 'an almost',\n",
       " 'an appearance',\n",
       " 'an artists',\n",
       " 'an assistant',\n",
       " 'an astonishing',\n",
       " 'an emphasis',\n",
       " 'an enormously',\n",
       " 'an ensemble',\n",
       " 'an epic',\n",
       " 'an exciting',\n",
       " 'an exhausting',\n",
       " 'an icy',\n",
       " 'an illusion',\n",
       " 'an important',\n",
       " 'an international',\n",
       " 'an irish',\n",
       " 'an ode',\n",
       " 'an older',\n",
       " 'an onscreen',\n",
       " 'an orange',\n",
       " 'an oscar',\n",
       " 'an underwater',\n",
       " 'ancient',\n",
       " 'ancient greeks',\n",
       " 'and',\n",
       " 'and 2001s',\n",
       " 'and after',\n",
       " 'and alexis',\n",
       " 'and allowing',\n",
       " 'and almost',\n",
       " 'and also',\n",
       " 'and although',\n",
       " 'and american',\n",
       " 'and an',\n",
       " 'and angular',\n",
       " 'and approached',\n",
       " 'and arif',\n",
       " 'and berg',\n",
       " 'and bossa',\n",
       " 'and brings',\n",
       " 'and browns',\n",
       " 'and budgets',\n",
       " 'and cg',\n",
       " 'and creating',\n",
       " 'and david',\n",
       " 'and disappearing',\n",
       " 'and donnie',\n",
       " 'and doris',\n",
       " 'and dozens',\n",
       " 'and dramatic',\n",
       " 'and eager',\n",
       " 'and early',\n",
       " 'and earnest',\n",
       " 'and entirely',\n",
       " 'and established',\n",
       " 'and eva',\n",
       " 'and even',\n",
       " 'and exotic',\n",
       " 'and extralong',\n",
       " 'and eyes',\n",
       " 'and falsetto',\n",
       " 'and family',\n",
       " 'and fear',\n",
       " 'and friendly',\n",
       " 'and funeral',\n",
       " 'and gifted',\n",
       " 'and glint',\n",
       " 'and halfassed',\n",
       " 'and herded',\n",
       " 'and his',\n",
       " 'and hooks',\n",
       " 'and hugh',\n",
       " 'and ian',\n",
       " 'and if',\n",
       " 'and impeccable',\n",
       " 'and in',\n",
       " 'and ineffectual',\n",
       " 'and irresistible',\n",
       " 'and its',\n",
       " 'and john',\n",
       " 'and kim',\n",
       " 'and lawrence',\n",
       " 'and leading',\n",
       " 'and lena',\n",
       " 'and martin',\n",
       " 'and mastered',\n",
       " 'and melodic',\n",
       " 'and memorable',\n",
       " 'and mum',\n",
       " 'and neoclassical',\n",
       " 'and nerverattling',\n",
       " 'and nothin',\n",
       " 'and now',\n",
       " 'and oasis',\n",
       " 'and often',\n",
       " 'and old',\n",
       " 'and only',\n",
       " 'and open',\n",
       " 'and others',\n",
       " 'and paternal',\n",
       " 'and patrick',\n",
       " 'and performances',\n",
       " 'and personal',\n",
       " 'and please',\n",
       " 'and pop',\n",
       " 'and preserves',\n",
       " 'and preteen',\n",
       " 'and producers',\n",
       " 'and production',\n",
       " 'and proper',\n",
       " 'and protect',\n",
       " 'and remained',\n",
       " 'and reused',\n",
       " 'and revenge',\n",
       " 'and romance',\n",
       " 'and romantic',\n",
       " 'and rome',\n",
       " 'and sam',\n",
       " 'and sandra',\n",
       " 'and satisfying',\n",
       " 'and seraph',\n",
       " 'and serious',\n",
       " 'and showed',\n",
       " 'and singer',\n",
       " 'and so',\n",
       " 'and sold',\n",
       " 'and something',\n",
       " 'and songwriter',\n",
       " 'and sonic',\n",
       " 'and sprinkle',\n",
       " 'and steve',\n",
       " 'and strategic',\n",
       " 'and streamlined',\n",
       " 'and successful',\n",
       " 'and summer',\n",
       " 'and sumptuous',\n",
       " 'and superbly',\n",
       " 'and supernatural',\n",
       " 'and support',\n",
       " 'and surprising',\n",
       " 'and surprisingly',\n",
       " 'and sweet',\n",
       " 'and syrupy',\n",
       " 'and takin',\n",
       " 'and teeth',\n",
       " 'and teleporting',\n",
       " 'and the',\n",
       " 'and their',\n",
       " 'and theres',\n",
       " 'and this',\n",
       " 'and thomas',\n",
       " 'and though',\n",
       " 'and trumpet',\n",
       " 'and turns',\n",
       " 'and vaughans',\n",
       " 'and vengeful',\n",
       " 'and was',\n",
       " 'and wellscrubbed',\n",
       " 'and while',\n",
       " 'and winding',\n",
       " 'and won',\n",
       " 'and wrapping',\n",
       " 'and you',\n",
       " 'and your',\n",
       " 'andrew',\n",
       " 'andrew leahey',\n",
       " 'andrew 经典',\n",
       " 'andy',\n",
       " 'andy 写给',\n",
       " 'andy 永远',\n",
       " 'angel1999',\n",
       " 'angular',\n",
       " 'angular particular',\n",
       " 'animal',\n",
       " 'animal fazenda',\n",
       " 'animals',\n",
       " 'animals teaming',\n",
       " 'animated',\n",
       " 'animated animals',\n",
       " 'animated roles',\n",
       " 'animation',\n",
       " 'animation offering',\n",
       " 'anita',\n",
       " 'anita baker',\n",
       " 'ann',\n",
       " 'ann brashares',\n",
       " 'anne',\n",
       " 'anne hepburn',\n",
       " 'another',\n",
       " 'another action',\n",
       " 'another highlight',\n",
       " 'another persons',\n",
       " 'another variation',\n",
       " 'answer',\n",
       " 'answer and',\n",
       " 'answer 您好',\n",
       " 'anti',\n",
       " 'any',\n",
       " 'any indication',\n",
       " 'any previous',\n",
       " 'any semblance',\n",
       " 'any sharp',\n",
       " 'any weight',\n",
       " 'anyway',\n",
       " 'anyway 音乐',\n",
       " 'aor',\n",
       " 'aor vets',\n",
       " 'aor 風格',\n",
       " 'apart',\n",
       " 'apart from',\n",
       " 'ape',\n",
       " 'ape 格式',\n",
       " 'ape 电脑',\n",
       " 'api',\n",
       " 'api 程序设计',\n",
       " 'api 里面',\n",
       " 'apologize',\n",
       " 'apologize this',\n",
       " 'appeal',\n",
       " 'appeal also',\n",
       " 'appealing',\n",
       " 'appealing friends',\n",
       " 'appealing gregory',\n",
       " 'appearance',\n",
       " 'appearance in',\n",
       " 'appearance was',\n",
       " 'appearances',\n",
       " 'appearances help',\n",
       " 'applied',\n",
       " 'applied sparingly',\n",
       " 'approach',\n",
       " 'approach may',\n",
       " 'approach to',\n",
       " 'approached',\n",
       " 'approached sans',\n",
       " 'appropriate',\n",
       " 'appropriate on',\n",
       " 'approving',\n",
       " 'approving as',\n",
       " 'arabia',\n",
       " 'arabia live',\n",
       " 'arbitrarily',\n",
       " 'arbitrarily cut',\n",
       " 'arc',\n",
       " 'arc of',\n",
       " 'arcady',\n",
       " 'arcady ran',\n",
       " 'arctic',\n",
       " 'arctic monkeys',\n",
       " 'are',\n",
       " 'are all',\n",
       " 'are also',\n",
       " 'are as',\n",
       " 'are bypassed',\n",
       " 'are cyclic',\n",
       " 'are dead',\n",
       " 'are difficult',\n",
       " 'are exceptional',\n",
       " 'are here',\n",
       " 'are impressive',\n",
       " 'are invariably',\n",
       " 'are motivated',\n",
       " 'are not',\n",
       " 'are primarily',\n",
       " 'are quietly',\n",
       " 'are refined',\n",
       " 'are slyly',\n",
       " 'are subtle',\n",
       " 'are the',\n",
       " 'are well',\n",
       " 'aretha',\n",
       " 'aretha franklin',\n",
       " 'argenteuil',\n",
       " 'argenteuil fine',\n",
       " 'arguably',\n",
       " 'arguably his',\n",
       " 'arif',\n",
       " 'arif mardin',\n",
       " 'aristocratic',\n",
       " 'aristocratic bale',\n",
       " 'ark',\n",
       " 'ark did',\n",
       " 'arrangements',\n",
       " 'arrangements or',\n",
       " 'arrive',\n",
       " 'arrive on',\n",
       " 'arsenal',\n",
       " 'arsenal of',\n",
       " 'art',\n",
       " 'art rock',\n",
       " 'artists',\n",
       " 'artists like',\n",
       " 'artists music',\n",
       " 'as',\n",
       " 'as always',\n",
       " 'as an',\n",
       " 'as average',\n",
       " 'as bonus',\n",
       " 'as brighteyed',\n",
       " 'as can',\n",
       " 'as chris',\n",
       " 'as clear',\n",
       " 'as contemporary',\n",
       " 'as desperately',\n",
       " 'as did',\n",
       " 'as dirty',\n",
       " 'as early80s',\n",
       " 'as he',\n",
       " 'as highly',\n",
       " 'as his',\n",
       " 'as if',\n",
       " 'as in',\n",
       " 'as is',\n",
       " ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
